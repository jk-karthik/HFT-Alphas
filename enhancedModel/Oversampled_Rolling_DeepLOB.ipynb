{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfcc0f65-21cd-415a-be98-1100cbcb8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import copy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad1c745-8666-408b-9d1d-1749283ebced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-14.6-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2201d5a7-db82-4632-aa0c-950d5b535959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/jandh/Desktop/Old Desktop/od/1 quater/Project Lab/labelled_data_500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27e7d585-b6c3-4ef7-830f-d3e4b401f75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2021-06-01', '2021-06-02', '2021-06-03', '2021-06-04',\n",
       "               '2021-06-07', '2021-06-08', '2021-06-09', '2021-06-10',\n",
       "               '2021-06-11', '2021-06-15', '2021-06-16', '2021-06-17',\n",
       "               '2021-06-18', '2021-06-21', '2021-06-22'],\n",
       "              dtype='datetime64[ns]', name='date', freq=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.to_datetime(df['date']).value_counts()\n",
    "x.sort_index().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc909887-dc4c-45b3-b811-a8f1d342e176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>SP1</th>\n",
       "      <th>SV1</th>\n",
       "      <th>BP1</th>\n",
       "      <th>BV1</th>\n",
       "      <th>SP2</th>\n",
       "      <th>SV2</th>\n",
       "      <th>BP2</th>\n",
       "      <th>BV2</th>\n",
       "      <th>SP3</th>\n",
       "      <th>...</th>\n",
       "      <th>BV3</th>\n",
       "      <th>SP4</th>\n",
       "      <th>SV4</th>\n",
       "      <th>BP4</th>\n",
       "      <th>BV4</th>\n",
       "      <th>SP5</th>\n",
       "      <th>SV5</th>\n",
       "      <th>BP5</th>\n",
       "      <th>BV5</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>10200.0</td>\n",
       "      <td>3294</td>\n",
       "      <td>10150.0</td>\n",
       "      <td>61</td>\n",
       "      <td>10250.0</td>\n",
       "      <td>2185</td>\n",
       "      <td>10100.0</td>\n",
       "      <td>1240</td>\n",
       "      <td>10300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1403</td>\n",
       "      <td>10350.0</td>\n",
       "      <td>2138</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2195</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>2596</td>\n",
       "      <td>9990.0</td>\n",
       "      <td>685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>10200.0</td>\n",
       "      <td>3294</td>\n",
       "      <td>10150.0</td>\n",
       "      <td>59</td>\n",
       "      <td>10250.0</td>\n",
       "      <td>2185</td>\n",
       "      <td>10100.0</td>\n",
       "      <td>1240</td>\n",
       "      <td>10300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1403</td>\n",
       "      <td>10350.0</td>\n",
       "      <td>2138</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2195</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>2596</td>\n",
       "      <td>9990.0</td>\n",
       "      <td>685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>10200.0</td>\n",
       "      <td>3324</td>\n",
       "      <td>10150.0</td>\n",
       "      <td>59</td>\n",
       "      <td>10250.0</td>\n",
       "      <td>2185</td>\n",
       "      <td>10100.0</td>\n",
       "      <td>1240</td>\n",
       "      <td>10300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1403</td>\n",
       "      <td>10350.0</td>\n",
       "      <td>2138</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2195</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>2596</td>\n",
       "      <td>9990.0</td>\n",
       "      <td>685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>10200.0</td>\n",
       "      <td>3324</td>\n",
       "      <td>10150.0</td>\n",
       "      <td>43</td>\n",
       "      <td>10250.0</td>\n",
       "      <td>2185</td>\n",
       "      <td>10100.0</td>\n",
       "      <td>1240</td>\n",
       "      <td>10300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1403</td>\n",
       "      <td>10350.0</td>\n",
       "      <td>2138</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2195</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>2596</td>\n",
       "      <td>9990.0</td>\n",
       "      <td>685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>10200.0</td>\n",
       "      <td>3324</td>\n",
       "      <td>10150.0</td>\n",
       "      <td>43</td>\n",
       "      <td>10250.0</td>\n",
       "      <td>2185</td>\n",
       "      <td>10100.0</td>\n",
       "      <td>1240</td>\n",
       "      <td>10300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1403</td>\n",
       "      <td>10350.0</td>\n",
       "      <td>2138</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2195</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>2596</td>\n",
       "      <td>9990.0</td>\n",
       "      <td>687</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      SP1   SV1      BP1  BV1      SP2   SV2      BP2   BV2  \\\n",
       "0  2021-06-01  10200.0  3294  10150.0   61  10250.0  2185  10100.0  1240   \n",
       "1  2021-06-01  10200.0  3294  10150.0   59  10250.0  2185  10100.0  1240   \n",
       "2  2021-06-01  10200.0  3324  10150.0   59  10250.0  2185  10100.0  1240   \n",
       "3  2021-06-01  10200.0  3324  10150.0   43  10250.0  2185  10100.0  1240   \n",
       "4  2021-06-01  10200.0  3324  10150.0   43  10250.0  2185  10100.0  1240   \n",
       "\n",
       "       SP3  ...   BV3      SP4   SV4      BP4   BV4      SP5   SV5     BP5  \\\n",
       "0  10300.0  ...  1403  10350.0  2138  10000.0  2195  10400.0  2596  9990.0   \n",
       "1  10300.0  ...  1403  10350.0  2138  10000.0  2195  10400.0  2596  9990.0   \n",
       "2  10300.0  ...  1403  10350.0  2138  10000.0  2195  10400.0  2596  9990.0   \n",
       "3  10300.0  ...  1403  10350.0  2138  10000.0  2195  10400.0  2596  9990.0   \n",
       "4  10300.0  ...  1403  10350.0  2138  10000.0  2195  10400.0  2596  9990.0   \n",
       "\n",
       "   BV5  label  \n",
       "0  685      1  \n",
       "1  685      1  \n",
       "2  685      1  \n",
       "3  685      1  \n",
       "4  687      1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = df[pd.to_datetime(df['date']) <= pd.to_datetime('2021-06-07')]\n",
    "test_data = df[pd.to_datetime(df['date']) == pd.to_datetime('2021-06-08')]\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c988eb1-473a-43eb-9a98-b1576711fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  train_set.iloc[:int(np.floor(train_set.shape[0] * 0.8)),:]\n",
    "eval_data = train_set.iloc[int(np.floor(train_set.shape[0] * 0.8)):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "414a4eb0-5b1c-45d0-b1d5-1478792c72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df,cols,norm):\n",
    "    #Normalizing using z-score\n",
    "    if norm=='Z':\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df[cols])\n",
    "        data = scaler.transform(df[cols])\n",
    "    \n",
    "    \n",
    "    #Normalizing using DecPrec\n",
    "    if norm=='DecPrec':\n",
    "        k_len = np.ceil(np.log10(df[cols].abs().max()))\n",
    "        # print(k_len)\n",
    "        data = df[cols]/(10**k_len)\n",
    "\n",
    "    return data,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5844b5e5-85ae-4751-b6cc-a6de8665df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input , scaler = normalize_data(train_data.iloc[:,1:-1],train_data.iloc[:,1:-1].columns,'Z')\n",
    "train_label = train_data.iloc[:,-1].to_numpy()\n",
    "eval_input = scaler.transform(eval_data.iloc[:,1:-1])\n",
    "eval_label = eval_data.iloc[:,-1].to_numpy()\n",
    "test_input = scaler.transform(test_data.iloc[:,1:-1])\n",
    "test_label = test_data.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c7cbbe-1d5f-4dbc-abd5-20c2fc4e3112",
   "metadata": {},
   "outputs": [],
   "source": [
    "del [train_data,test_data,eval_data,df,train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df032418-37fd-4b82-b473-94ff770f3de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015896 mb\n"
     ]
    }
   ],
   "source": [
    "s=0\n",
    "cc=0\n",
    "key=0\n",
    "obj=0\n",
    "for key,obj in locals().items():\n",
    "    cc= sys.getsizeof(obj)/1000000\n",
    "    s+=cc\n",
    "    if cc>20:\n",
    "        print(key,cc)\n",
    "print(s,'mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c923fce7-68c0-460d-9d08-37cd7d5233bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(eval_input).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c558ad-1379-411e-b77e-3f28f5b05a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_LOB(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, x,y, num_classes, T):\n",
    "        \"\"\"Initialization\"\"\" \n",
    "        # self.k = k\n",
    "        self.num_classes = num_classes\n",
    "        self.T = T\n",
    "            \n",
    "        # x = prepare_x(data)\n",
    "        # y = get_label(data)\n",
    "        x, y = data_classification(x, y, self.T)\n",
    "        # y = y[:,self.k] - 1\n",
    "        self.length = len(x)\n",
    "\n",
    "        x = torch.from_numpy(x)\n",
    "        self.x = torch.unsqueeze(x, 1)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates samples of data\"\"\"\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d81ca37-fb4b-4ff5-8c29-8fa4b3490c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_classification(X, Y, T):\n",
    "    [N, D] = X.shape\n",
    "    # print(X.shape,T,N,Y.shape)\n",
    "    df = np.array(X)\n",
    "    # print(df.shape)\n",
    "    dY = np.array(Y)\n",
    "\n",
    "    dataY = dY[T - 1:N]\n",
    "\n",
    "    dataX = np.zeros((N - T + 1, T, D))\n",
    "    for i in range(T, N + 1):\n",
    "        dataX[i - T] = df[i - T:i, :]\n",
    "\n",
    "    return dataX, dataY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6e96f4f-6d62-40d1-8445-250ad5f99b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataset_train = Dataset_LOB(train_input,train_label, num_classes=3, T=100)\n",
    "dataset_eval = Dataset_LOB(eval_input,eval_label, num_classes=3, T=100)\n",
    "dataset_test = Dataset_LOB(test_input,test_label, num_classes=3, T=100)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aa809e0-d6ab-40e9-a1fb-4a3c1faa01bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1732671, 1, 100, 20]) torch.Size([282495])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_eval, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "print(dataset_train.x.shape, dataset_test.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3de51a24-8d06-4caa-8d8f-f42a8a8e4003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.9308, -0.8022,  0.9326,  ..., -0.4754,  0.9477, -0.1733],\n",
      "          [ 0.9308, -0.8022,  0.9326,  ..., -0.4754,  0.9477, -0.1733],\n",
      "          [ 0.9308, -0.8022,  0.9326,  ..., -0.4754,  0.9477, -0.1733],\n",
      "          ...,\n",
      "          [ 0.9308, -0.7027,  0.9326,  ..., -0.5190,  0.9477, -0.1687],\n",
      "          [ 0.9308, -0.7011,  0.9326,  ..., -0.5190,  0.9477, -0.1687],\n",
      "          [ 0.9308, -0.7011,  0.9326,  ..., -0.5190,  0.9477, -0.1687]]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([2])\n",
      "torch.Size([1, 1, 100, 20]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "tmp_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=1, shuffle=True)\n",
    "\n",
    "for x, y in tmp_loader:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d7e52b9-a788-4f79-8a55-736bed775e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "device = mps_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4f5eefc-1aa2-4193-8b24-974d9a2c3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deeplob(nn.Module):\n",
    "    def __init__(self, y_len):\n",
    "        super().__init__()\n",
    "        self.y_len = y_len\n",
    "        \n",
    "        # convolution blocks\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "#             nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,5)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        \n",
    "        # inception moduels\n",
    "        self.inp1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp3 = nn.Sequential(\n",
    "            nn.MaxPool2d((3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        \n",
    "        # lstm layers\n",
    "        self.lstm = nn.LSTM(input_size=192, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, self.y_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h0: (number of hidden layers, batch size, hidden size)\n",
    "        h0 = torch.zeros(1, x.size(0), 64).to(device)\n",
    "        c0 = torch.zeros(1, x.size(0), 64).to(device)\n",
    "    \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x_inp1 = self.inp1(x)\n",
    "        x_inp2 = self.inp2(x)\n",
    "        x_inp3 = self.inp3(x)  \n",
    "        \n",
    "        x = torch.cat((x_inp1, x_inp2, x_inp3), dim=1)\n",
    "        \n",
    "#         x = torch.transpose(x, 1, 2)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = torch.reshape(x, (-1, x.shape[1], x.shape[2]))\n",
    "        \n",
    "        x, _ = self.lstm(x, (h0, c0))\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc1(x)\n",
    "        forecast_y = torch.softmax(x, dim=1)\n",
    "        \n",
    "        return forecast_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68787ae3-0c55-426f-8132-51f669a79588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deeplob(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(1, 2), stride=(1, 2))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(1, 2), stride=(1, 2))\n",
       "    (1): Tanh()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (4): Tanh()\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (7): Tanh()\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(32, 32, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (inp1): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=same)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (inp2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(5, 1), stride=(1, 1), padding=same)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (inp3): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
       "    (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lstm): LSTM(192, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = deeplob(y_len = dataset_train.num_classes)\n",
    "model.to(mps_device)\n",
    "# device = mps_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8daad7a-acf1-4aa5-8b86-303a38d2d3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15.6191,  1.1393, 17.1709])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count = Counter(train_label)\n",
    "class_weights = torch.Tensor([len(train_label)/c for c in pd.Series(class_count).sort_index().values])\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e12f5314-8932-463f-aa4c-c0952c9bcd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23.4287,  1.1393, 25.7564])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights[0]*=1.5\n",
    "class_weights[2]*=1.5\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e775062e-d6e7-4789-aad5-3c6092fa82a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weights.to(device)  \n",
    "criterion = nn.CrossEntropyLoss(class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a2e33f2-7b76-4da0-8ce4-f8512e9ea045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=1, path='./model.pt'):\n",
    "        self.patience = patience\n",
    "        self.path= path\n",
    "        self.counter = 0\n",
    "        self.best_score = np.inf\n",
    "        self.early_stop = False\n",
    "        self.best_test_epoch = 0\n",
    "        \n",
    "    def __call__(self, val_loss, model,it):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            self.best_test_epoch = it\n",
    "    \n",
    "        elif val_loss > self.best_score:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True \n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "            self.best_test_epoch = it\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f11528ad-c3d8-47ba-9361-29c1517b3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/Users/jandh/Desktop/Old Desktop/od/1 quater/Project Lab/fast-alphas/best_val_roLOB_oversampled.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41fa8cf3-a044-443d-9560-2c83018f2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "        patience=7, \n",
    "        path=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be30c776-77d7-4e28-a2df-4465afeb613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):\n",
    "\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "        \n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for inputs, targets in train_loader:\n",
    "            # move data to GPU\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            # print(\"about to get model output\")\n",
    "            outputs = model(inputs)\n",
    "            # print(\"done getting model output\")\n",
    "            # print(\"outputs.shape:\", outputs.shape, \"targets.shape:\", targets.shape)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward and optimize\n",
    "            # print(\"about to optimize\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        # Get train loss and test loss\n",
    "        train_loss = np.mean(train_loss) # a little misleading\n",
    "    \n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)      \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss.append(loss.item())\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "        \n",
    "        early_stopping(test_loss,model,it)\n",
    "        if early_stopping.early_stop:\n",
    "            print('Patience Exhausted')\n",
    "            break\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(dt)\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "          Validation Loss: {test_loss:.4f}, Duration: {dt}, Best Val Epoch: {early_stopping.best_test_epoch}')\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c1cf47f-0aec-42f6-ac7d-2b06857bb742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██                                      | 1/20 [25:37<8:06:44, 1537.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:25:37.096070\n",
      "Epoch 1/20, Train Loss: 0.7717,           Validation Loss: 1.0817, Duration: 0:25:37.096070, Best Val Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████                                    | 2/20 [52:39<7:56:13, 1587.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:02.644952\n",
      "Epoch 2/20, Train Loss: 0.7045,           Validation Loss: 1.0807, Duration: 0:27:02.644952, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▋                                | 3/20 [1:19:56<7:36:10, 1610.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:16.958219\n",
      "Epoch 3/20, Train Loss: 0.6828,           Validation Loss: 1.0849, Duration: 0:27:16.958219, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▌                              | 4/20 [1:47:17<7:12:34, 1622.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:20.759373\n",
      "Epoch 4/20, Train Loss: 0.6708,           Validation Loss: 1.1106, Duration: 0:27:20.759373, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▌                            | 5/20 [2:14:39<6:47:20, 1629.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:22.119368\n",
      "Epoch 5/20, Train Loss: 0.6611,           Validation Loss: 1.0849, Duration: 0:27:22.119368, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████▍                          | 6/20 [2:41:54<6:20:37, 1631.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:14.880724\n",
      "Epoch 6/20, Train Loss: 0.6527,           Validation Loss: 1.1037, Duration: 0:27:14.880724, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████▎                        | 7/20 [3:09:12<5:53:56, 1633.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:18.272007\n",
      "Epoch 7/20, Train Loss: 0.6459,           Validation Loss: 1.0370, Duration: 0:27:18.272007, Best Val Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████▏                      | 8/20 [3:36:33<5:27:08, 1635.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:20.358931\n",
      "Epoch 8/20, Train Loss: 0.6435,           Validation Loss: 1.0945, Duration: 0:27:20.358931, Best Val Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████                     | 9/20 [4:03:53<5:00:07, 1637.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:19.993133\n",
      "Epoch 9/20, Train Loss: 0.6397,           Validation Loss: 1.0237, Duration: 0:27:19.993133, Best Val Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████▌                  | 10/20 [4:31:19<4:33:19, 1639.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:26.423472\n",
      "Epoch 10/20, Train Loss: 0.6380,           Validation Loss: 1.0028, Duration: 0:27:26.423472, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████▎                | 11/20 [4:58:40<4:06:03, 1640.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:21.288795\n",
      "Epoch 11/20, Train Loss: 0.6344,           Validation Loss: 1.0231, Duration: 0:27:21.288795, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████▏              | 12/20 [5:26:06<3:38:55, 1641.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:25.238232\n",
      "Epoch 12/20, Train Loss: 0.6317,           Validation Loss: 1.0862, Duration: 0:27:25.238232, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████             | 13/20 [5:53:21<3:11:19, 1639.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:15.471926\n",
      "Epoch 13/20, Train Loss: 0.6269,           Validation Loss: 1.0457, Duration: 0:27:15.471926, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████▉           | 14/20 [6:20:45<2:44:06, 1641.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:23.637765\n",
      "Epoch 14/20, Train Loss: 0.6268,           Validation Loss: 1.0177, Duration: 0:27:23.637765, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████▊         | 15/20 [6:48:03<2:16:40, 1640.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:18.117976\n",
      "Epoch 15/20, Train Loss: 0.6248,           Validation Loss: 1.0493, Duration: 0:27:18.117976, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████▌       | 16/20 [7:15:23<1:49:20, 1640.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:27:19.584215\n",
      "Epoch 16/20, Train Loss: 0.6242,           Validation Loss: 1.0216, Duration: 0:27:19.584215, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████▌       | 16/20 [7:42:35<1:55:38, 1734.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience Exhausted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = batch_gd(model, criterion, optimizer, \n",
    "                                    train_loader, val_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868dd91-d9d7-4006-b786-0e77c4cedea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336327d4-f4b0-4657-b979-f2e0ac67f52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58734b-cdd1-40dc-b1c8-14ca2f26b63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db98ecb2-9d51-457e-bc77-8c123a72c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.5823\n"
     ]
    }
   ],
   "source": [
    "n_correct = 0.\n",
    "n_total = 0.\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    model.eval()\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    # update counts\n",
    "    n_correct += (predictions == targets).sum().item()\n",
    "    n_total += targets.shape[0]\n",
    "\n",
    "    all_targets.append(targets.cpu().numpy())\n",
    "    all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "test_acc = n_correct / n_total\n",
    "print(f\"Test acc: {test_acc:.4f}\")\n",
    "\n",
    "all_targets = np.concatenate(all_targets)    \n",
    "all_predictions = np.concatenate(all_predictions)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67db6e19-8138-45a2-8187-c71ba0d33c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.582293491920211\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1730    0.4108    0.2435     22858\n",
      "           1     0.8829    0.6081    0.7202    240272\n",
      "           2     0.1434    0.4644    0.2191     19365\n",
      "\n",
      "    accuracy                         0.5823    282495\n",
      "   macro avg     0.3998    0.4944    0.3943    282495\n",
      "weighted avg     0.7748    0.5823    0.6473    282495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cb82a6f-5739-4e8e-b0c9-deea96ebc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/Users/jandh/Desktop/Old Desktop/od/1 quater/Project Lab/model_oversampled.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d06a6-264c-437f-93ac-cb73fd38228b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a6340c6-080c-4a07-b0a0-17aa2cf6d54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vf/gw1t6f4j3mv84kp8kpw4j_bw0000gn/T/ipykernel_57021/1991610670.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/Users/jandh/Desktop/Old Desktop/od/1 quater/Project Lab/fast-alphas/best_val_roLOB_oversampled.pt', map_location=mps_device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/Users/jandh/Desktop/Old Desktop/od/1 quater/Project Lab/fast-alphas/best_val_roLOB_oversampled.pt', map_location=mps_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25609ce5-64b0-4acc-b498-0fe6b9e52eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.5360\n"
     ]
    }
   ],
   "source": [
    "n_correct = 0.\n",
    "n_total = 0.\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    model.eval()\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    # update counts\n",
    "    n_correct += (predictions == targets).sum().item()\n",
    "    n_total += targets.shape[0]\n",
    "\n",
    "    all_targets.append(targets.cpu().numpy())\n",
    "    all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "test_acc = n_correct / n_total\n",
    "print(f\"Test acc: {test_acc:.4f}\")\n",
    "\n",
    "all_targets = np.concatenate(all_targets)    \n",
    "all_predictions = np.concatenate(all_predictions)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9f074be-8344-4921-8ba7-aaa23ad0542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.5359988672365883\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1610    0.4797    0.2411     22858\n",
      "           1     0.8883    0.5414    0.6728    240272\n",
      "           2     0.1526    0.5352    0.2375     19365\n",
      "\n",
      "    accuracy                         0.5360    282495\n",
      "   macro avg     0.4006    0.5188    0.3838    282495\n",
      "weighted avg     0.7790    0.5360    0.6080    282495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f214245-22ff-4aff-a9be-b80c14a37235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
